# -*- coding: utf-8 -*-
"""Decision_TreeMod1VSMod2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SYCZbVGnRJBopmAPu-OZFNggup7bxbHX
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#Loading boston dataset
boston=pd.read_csv("BostonHousing.csv")

#calculate number of rows and columns
boston.shape

#scatterplot
plt.scatter(x=boston['rm'],y=boston['medv'],color='brown')
plt.xlabel('Avg no. of rooms per dwelling')
plt.ylabel('Median value of home')

# getting the features and target
x=pd.DataFrame(boston['rm']) #feature
y=pd.DataFrame(boston['medv']) #target

#splitting into train and test
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20)  #test_size=0.20 means 20% of the data is for test and rest 80% for train

#Building the model
from sklearn.tree import DecisionTreeRegressor
regressor=DecisionTreeRegressor()
regressor.fit(x_train,y_train)

y_pred=regressor.predict(x_test)

#finding the rmse ie root mean squared error value
from sklearn.metrics import mean_squared_error
mse=mean_squared_error(y_pred,y_test)
rmse=np.sqrt(mse)
rmse

# Model 1 with one dependent feature ie 'rm' rmse=7.67 now lets check for more dependent features

#getting the features and target
x=pd.DataFrame(boston[['rm','lstat','age']]) #feature
y=pd.DataFrame(boston['medv']) #target
#splitting into train and test
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30)  #test_size=0.20 means 20% of the data is for test and rest 80% for train
regressor=DecisionTreeRegressor()
regressor.fit(x_train,y_train)
y_pred=regressor.predict(x_test)
mse=mean_squared_error(y_pred,y_test)
rmse=np.sqrt(mse)
rmse
# Model 2 with three dependent feature ie 'rm' 'lstat','age' rmse=6.037 which is very less error than model 1

